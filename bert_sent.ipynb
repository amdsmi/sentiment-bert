{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRCqNnBbFxph",
        "outputId": "51601c7c-07b5-4bd6-b54c-8cd7120a1d60"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\r\u001b[K     |▏                               | 10kB 15.5MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 19.6MB/s eta 0:00:01\r\u001b[K     |▍                               | 30kB 22.2MB/s eta 0:00:01\r\u001b[K     |▌                               | 40kB 22.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 51kB 24.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 61kB 25.7MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 25.9MB/s eta 0:00:01\r\u001b[K     |█                               | 81kB 26.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 92kB 26.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 102kB 26.7MB/s eta 0:00:01\r\u001b[K     |█▍                              | 112kB 26.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 122kB 26.7MB/s eta 0:00:01\r\u001b[K     |█▊                              | 133kB 26.7MB/s eta 0:00:01\r\u001b[K     |█▉                              | 143kB 26.7MB/s eta 0:00:01\r\u001b[K     |██                              | 153kB 26.7MB/s eta 0:00:01\r\u001b[K     |██                              | 163kB 26.7MB/s eta 0:00:01\r\u001b[K     |██▎                             | 174kB 26.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 184kB 26.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 194kB 26.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 204kB 26.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 215kB 26.7MB/s eta 0:00:01\r\u001b[K     |██▉                             | 225kB 26.7MB/s eta 0:00:01\r\u001b[K     |███                             | 235kB 26.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 245kB 26.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 256kB 26.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 266kB 26.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 276kB 26.7MB/s eta 0:00:01\r\u001b[K     |███▊                            | 286kB 26.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 296kB 26.7MB/s eta 0:00:01\r\u001b[K     |████                            | 307kB 26.7MB/s eta 0:00:01\r\u001b[K     |████                            | 317kB 26.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 327kB 26.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 337kB 26.7MB/s eta 0:00:01\r\u001b[K     |████▌                           | 348kB 26.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 358kB 26.7MB/s eta 0:00:01\r\u001b[K     |████▊                           | 368kB 26.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 378kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 389kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 399kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 409kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 419kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 430kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 440kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 450kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 460kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 471kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 481kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 491kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 501kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 512kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 522kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 532kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 542kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 552kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 563kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 573kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 583kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 593kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 604kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 614kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 624kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 634kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 645kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 655kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 665kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 675kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 686kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 696kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 706kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 716kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 727kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 737kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 747kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 757kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 768kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 778kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 788kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 798kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 808kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 819kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 829kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 839kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 849kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 860kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 870kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 880kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 890kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 901kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 911kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 921kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 931kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 942kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 952kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 962kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 972kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 983kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 993kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 1.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.6MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.6MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.6MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.6MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.6MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.6MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.6MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.6MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.6MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.6MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.7MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.7MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.7MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.7MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.7MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.7MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.7MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.8MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.8MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.8MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.8MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.8MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.8MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.8MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.8MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.8MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.8MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.9MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.9MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.9MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.9MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.9MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.9MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.9MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.9MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.9MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.9MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 2.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 2.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.0MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 2.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 2.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.1MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 2.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.2MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 2.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.3MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.4MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.5MB 26.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.5MB 26.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 43.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 52.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh-CNq25GWS-",
        "outputId": "1ac703f1-a4c8-4af5-fe02-f5dd1cd4c19b"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import numpy as np\n",
        "from torchtext.legacy import data\n",
        "SEED = 1234\n",
        "from tqdm import tqdm\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "#%%\n",
        "dataset=pd.read_excel('/content/drive/MyDrive/hotel.xlsx')\n",
        "dataset.columns\n",
        "dataset=dataset[['Rating','Review']]\n",
        "for i in range(len(dataset)):\n",
        "  if dataset.Rating[i]>3:\n",
        "    dataset.Rating[i]='pos'\n",
        "  elif dataset.Rating[i]<3:\n",
        "    dataset.Rating[i]='neg'\n",
        "  else:\n",
        "    dataset.Rating[i]='ntr'\n",
        "\n",
        "df=dataset.sort_values('Rating')\n",
        "\n",
        "df1,df2=df[:20000],df[-20001:-1]\n",
        "\n",
        "my_data=result = pd.concat([df1,df2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiRI-7_QNUrW",
        "outputId": "6397b2c9-78c4-41a3-cf58-3e36a01e7dbc"
      },
      "source": [
        "print(sum(my_data.Rating=='pos'))\n",
        "print(sum(my_data.Rating=='neg'))\n",
        "print(sum(my_data.Rating=='ntr'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000\n",
            "20000\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7r4ujxmNm5V"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcEDArYdNq9p",
        "outputId": "a592cb66-105f-4a11-f452-59b18d390f02"
      },
      "source": [
        "len(tokenizer.vocab)\n",
        "\n",
        "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(indexes)\n",
        "\n",
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)\n",
        "\n",
        "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
        "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
        "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
        "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'world', 'how', 'are', 'you', '?']\n",
            "[7592, 2088, 2129, 2024, 2017, 1029]\n",
            "[CLS] [SEP] [PAD] [UNK]\n",
            "101 102 0 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2t3hn37N2rZ",
        "outputId": "589bbce2-48a7-4791-fd65-ac4c63aff7dd"
      },
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)\n",
        "\n",
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "print(max_input_length)\n",
        "\n",
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101 102 0 100\n",
            "512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmDrHsfzN9XT"
      },
      "source": [
        "my_data.to_csv('my_data.csv', index=False)\n",
        "\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "\n",
        "my_dataset= data.TabularDataset(\n",
        "    path='my_data.csv',\n",
        "    format='csv',\n",
        "    skip_header=True,\n",
        "    fields=[('Rating',LABEL),('Review',TEXT)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxD7YILaPl8M",
        "outputId": "51b26eaf-a730-4d93-f241-bf4e321255f4"
      },
      "source": [
        "train_data, test_data = my_dataset.split(random_state = random.seed(SEED))\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
        "\n",
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")\n",
        "\n",
        "print(vars(train_data.examples[6]))\n",
        "\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['Review'])\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "LABEL.build_vocab(train_data)\n",
        "print(LABEL.vocab.stoi)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator = data.BucketIterator(\n",
        "    (train_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)\n",
        "valid_iterator= data.BucketIterator(\n",
        "    (valid_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)\n",
        "test_iterator = data.BucketIterator(\n",
        "    (test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 19600\n",
            "Number of validation examples: 8400\n",
            "Number of testing examples: 12000\n",
            "{'Rating': 'pos', 'Review': [2023, 4825, 2003, 1996, 3819, 1057, 9333, 7987, 4609, 2818, 3962, 1012, 1996, 7254, 10747, 2003, 2307, 1012, 1045, 3641, 1996, 4880, 21202, 2072, 2029, 2003, 4850, 2606, 24857, 2007, 27940, 1010, 18414, 13344, 20130, 1998, 2712, 8040, 8095, 11923, 1012, 1996, 18414, 13344, 20130, 1010, 27940, 1998, 2712, 8040, 8095, 11923, 2020, 21305, 2007, 14894, 999, 2061, 12090, 1045, 1005, 1049, 26308, 3436, 2004, 1045, 1005, 1049, 22868, 2021, 1996, 24857, 2001, 5410, 1012, 2009, 1005, 1055, 7463, 1999, 1037, 27940, 22953, 2705, 2021, 1045, 2371, 2009, 2734, 11968, 7834, 2319, 2000, 5587, 1037, 2210, 5926, 1012, 1012, 1012, 1037, 2210, 2242, 1012, 2005, 18064, 1045, 3641, 1996, 14418, 10199, 9850, 1010, 2009, 2001, 2074, 2205, 4086, 2005, 2026, 16663, 1012, 2673, 2182, 2001, 2204, 1010, 1998, 1045, 2763, 2097, 2022, 2746, 2067, 2043, 1996, 19350, 10747, 7480, 1012, 1037, 1522, 1037, 2307, 3962, 10323, 1005, 1055, 999]}\n",
            "['this', 'restaurant', 'is', 'the', 'perfect', 'u', '##ws', 'br', '##un', '##ch', 'spot', '.', 'the', 'outdoor', 'seating', 'is', 'great', '.', 'i', 'ordered', 'the', 'cape', '##llin', '##i', 'which', 'is', 'angel', 'hair', 'pasta', 'with', 'lobster', ',', 'ju', '##mbo', 'shrimp', 'and', 'sea', 'sc', '##all', '##ops', '.', 'the', 'ju', '##mbo', 'shrimp', ',', 'lobster', 'and', 'sea', 'sc', '##all', '##ops', 'were', 'bursting', 'with', 'flavor', '!', 'so', 'delicious', 'i', \"'\", 'm', 'saliva', '##ting', 'as', 'i', \"'\", 'm', 'typing', 'but', 'the', 'pasta', 'was', 'weak', '.', 'it', \"'\", 's', 'tossed', 'in', 'a', 'lobster', 'bro', '##th', 'but', 'i', 'felt', 'it', 'needed', 'par', '##mes', '##an', 'to', 'add', 'a', 'little', 'kick', '.', '.', '.', 'a', 'little', 'something', '.', 'for', 'dessert', 'i', 'ordered', 'the', 'cara', '##mel', 'cake', ',', 'it', 'was', 'just', 'too', 'sweet', 'for', 'my', 'liking', '.', 'everything', 'here', 'was', 'good', ',', 'and', 'i', 'probably', 'will', 'be', 'coming', 'back', 'when', 'the', 'outdoors', 'seating', 'opens', '.', 'a', '‚', 'a', 'great', 'spot', 'isabella', \"'\", 's', '!']\n",
            "defaultdict(None, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDSD9OJSQf5x",
        "outputId": "891903bb-508b-4fbf-a923-97a8f4fbe678"
      },
      "source": [
        "from tqdm import tqdm\n",
        "class BERTGRUSentiment(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = bert\n",
        "\n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "\n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers=n_layers,\n",
        "                          bidirectional=bidirectional,\n",
        "                          batch_first=True,\n",
        "                          dropout=0 if n_layers < 2 else dropout)\n",
        "\n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        # text = [batch size, sent len]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]\n",
        "\n",
        "        # embedded = [batch size, sent len, emb dim]\n",
        "\n",
        "        _, hidden = self.rnn(embedded)\n",
        "\n",
        "        # hidden = [n layers * n directions, batch size, emb dim]\n",
        "\n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1, :, :])\n",
        "\n",
        "        # hidden = [batch size, hid dim]\n",
        "\n",
        "        output = self.out(hidden)\n",
        "\n",
        "        # output = [batch size, out dim]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "\n",
        "model = BERTGRUSentiment(bert,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                         N_LAYERS,\n",
        "                         BIDIRECTIONAL,\n",
        "                         DROPOUT)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(iterator):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(batch.Review).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.Rating)\n",
        "\n",
        "        acc = binary_accuracy(predictions, batch.Rating)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(iterator):\n",
        "            predictions = model(batch.Review).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions, batch.Rating)\n",
        "\n",
        "            acc = binary_accuracy(predictions, batch.Rating)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 112,241,409 trainable parameters\n",
            "The model has 2,759,169 trainable parameters\n",
            "rnn.weight_ih_l0\n",
            "rnn.weight_hh_l0\n",
            "rnn.bias_ih_l0\n",
            "rnn.bias_hh_l0\n",
            "rnn.weight_ih_l0_reverse\n",
            "rnn.weight_hh_l0_reverse\n",
            "rnn.bias_ih_l0_reverse\n",
            "rnn.bias_hh_l0_reverse\n",
            "rnn.weight_ih_l1\n",
            "rnn.weight_hh_l1\n",
            "rnn.bias_ih_l1\n",
            "rnn.bias_hh_l1\n",
            "rnn.weight_ih_l1_reverse\n",
            "rnn.weight_hh_l1_reverse\n",
            "rnn.bias_ih_l1_reverse\n",
            "rnn.bias_hh_l1_reverse\n",
            "out.weight\n",
            "out.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rogikXXlim1",
        "outputId": "208cea27-ca01-4786-de27-f0838fa93933"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 154/154 [15:58<00:00,  6.22s/it]\n",
            "100%|██████████| 66/66 [06:13<00:00,  5.65s/it]\n",
            "  0%|          | 0/154 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 22m 11s\n",
            "\tTrain Loss: 0.173 | Train Acc: 93.30%\n",
            "\t Val. Loss: 0.073 |  Val. Acc: 97.50%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 154/154 [15:53<00:00,  6.19s/it]\n",
            "100%|██████████| 66/66 [06:18<00:00,  5.73s/it]\n",
            "  0%|          | 0/154 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Epoch Time: 22m 11s\n",
            "\tTrain Loss: 0.085 | Train Acc: 96.88%\n",
            "\t Val. Loss: 0.070 |  Val. Acc: 97.57%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 154/154 [15:53<00:00,  6.19s/it]\n",
            "100%|██████████| 66/66 [06:15<00:00,  5.68s/it]\n",
            "  0%|          | 0/154 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Epoch Time: 22m 9s\n",
            "\tTrain Loss: 0.062 | Train Acc: 97.79%\n",
            "\t Val. Loss: 0.086 |  Val. Acc: 97.35%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 154/154 [15:55<00:00,  6.21s/it]\n",
            "100%|██████████| 66/66 [06:15<00:00,  5.69s/it]\n",
            "  0%|          | 0/154 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Epoch Time: 22m 11s\n",
            "\tTrain Loss: 0.055 | Train Acc: 98.11%\n",
            "\t Val. Loss: 0.086 |  Val. Acc: 96.69%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 154/154 [15:57<00:00,  6.22s/it]\n",
            "100%|██████████| 66/66 [06:14<00:00,  5.67s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Epoch Time: 22m 11s\n",
            "\tTrain Loss: 0.044 | Train Acc: 98.41%\n",
            "\t Val. Loss: 0.056 |  Val. Acc: 98.04%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCKcMrH1mBVr"
      },
      "source": [
        "model.load_state_dict(torch.load('tut6-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk4yJSikolvQ"
      },
      "source": [
        "def predict_sentiment(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxVH6xmgomow"
      },
      "source": [
        "predict_sentiment(model, tokenizer, \"This hotel is terrible\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}